{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "challenging-eight",
   "metadata": {},
   "source": [
    "This is the answers for the questions listed in DMBA Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-experiment",
   "metadata": {},
   "source": [
    "2.1 - Assuming that data mining techniques are to be used in the following cases, identify whether the task required is supervised or unsupervised learning.\n",
    "\n",
    "a. Supervised\n",
    "b. Supervised\n",
    "c. Supervised\n",
    "d. Unsupervised\n",
    "e. Supervised\n",
    "f. Supervised\n",
    "g. Unsupervised\n",
    "h. Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-coordinator",
   "metadata": {},
   "source": [
    "2.2 - Describe the difference in roles assumed by the validation partition and the test partition.\n",
    "\n",
    "Validation partition hold the role of testing the initial accuracy and predictive capabilities of a model. Unlike, the validation partition, the test partition is new data that is used to further check the persomance of the predictive capabilities. Sometimes we create a model that has biases towards the validation partition so the test partition is check for these biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-ozone",
   "metadata": {},
   "source": [
    "2.3 - Consider the sample from a database of credit applicants in Table 2.16. Comment on the likelihood that it was sampled randomly, and whether it is likely to be a useful sample.\n",
    "\n",
    "The data sample was most likely not sampled randomly from the database as the OBS is incrementing in steps of 8 showing that this is every 8 OBS. This data will most likely not be a useful dataset because of lack of information as a whole. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-airfare",
   "metadata": {},
   "source": [
    "2.4 - Consider the sample from a bank database shown in Table 2.17; it was selected randomly from a larger database to be the training set. Personal Loan indicates whether a solicitation for a personal loan was accepted and is the response variable. A campaign is planned for a similar solicitation in the future and the bank is looking for a model that will identify likely responders. Examine the data carefully and indicate what your next step would be.\n",
    "\n",
    "The next step for this specific model would be to create a partition to be used for training and another to be used for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-senator",
   "metadata": {},
   "source": [
    "2.5 - Using the concept of overfitting, explain why when a model is fit to training data, zero error with those data is not necessarily good.\n",
    "\n",
    "A model that is fit to a training data with zero errors is not taking into consideration the actual predictive abilities of a model. Figure 2.2 in the book illustrated that if we fit a model to training data with a curve line having zero error that we will see that is is no way of correlating a correct move from the curve, instead a straight line would be more effective to find a better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-march",
   "metadata": {},
   "source": [
    "2.6 - In fitting a model to classify prospects as purchasers or nonpurchasers, a certain company drew the training data from internal data that include demographic and purchase information. Future data to be classified will be lists purchased from other sources, with demographic (but not purchase) data included. It was found that “refund issued” was a useful predictor in the training data. Why is this not an appropriate variable to include in the model?\n",
    "\n",
    "The variable of refund issues is not an appropriate variable to include in the model because future data will not include purchase data. Without having purchase data, refund data is irrelevent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-cloud",
   "metadata": {},
   "source": [
    "2.7 - A dataset has 1000 records and 50 variables with 5% of the values missing, spread randomly throughout the records and variables. An analyst decides to remove records with missing values. About how many records would you expect to be removed?\n",
    "\n",
    "If 5% of the values are missing in the dataset then we can assume that 5% of 1000 or 50 records would be removed, but this could be lower as a single record could me missing multiple variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "billion-practitioner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9230550247232868\n"
     ]
    }
   ],
   "source": [
    "#this was incorrect, correct answer is below\n",
    "correct = 1-(1-0.05)**50\n",
    "print(correct)\n",
    "#this meaning that ~923 records will contain a missing variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-india",
   "metadata": {},
   "source": [
    "2.8 - Normalize the data in Table 2.18, showing calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "obvious-contributor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  Income\n",
      "0   25   49000\n",
      "1   56  156000\n",
      "2   65   99000\n",
      "3   32  192000\n",
      "4   41   39000\n",
      "5   49   57000\n",
      "        Age    Income\n",
      "0 -1.438597 -0.865431\n",
      "1  0.829022  0.999021\n",
      "2  1.487363  0.005808\n",
      "3 -0.926554  1.626314\n",
      "4 -0.268213 -1.039679\n",
      "5  0.316979 -0.726033\n"
     ]
    }
   ],
   "source": [
    "#this is the better way of doing this\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#create a dataframe\n",
    "df = pd.DataFrame({\n",
    "    'Age': [25, 56, 65, 32, 41, 49],\n",
    "    'Income': [49000, 156000, 99000, 192000, 39000, 57000]\n",
    "})\n",
    "print(df)\n",
    "\n",
    "#normalizing data\n",
    "scaler = StandardScaler()\n",
    "df_norm = pd.DataFrame(scaler.fit_transform(df), index=df.index, columns=df.columns)\n",
    "print(df_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "warming-butterfly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98666.66666666667\n"
     ]
    }
   ],
   "source": [
    "#finding mean for calulations of income\n",
    "a = 49000\n",
    "b = 156000\n",
    "c = 99000\n",
    "d = 192000\n",
    "e = 39000\n",
    "f = 57000\n",
    "meanIncome = (a + b + c + d + e + f) / 6\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "opposed-joshua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118568000000.0\n"
     ]
    }
   ],
   "source": [
    "#finding varience for calulation of income \n",
    "tmp1 = (a - meanIncome)**2\n",
    "\n",
    "tmp2 = (b - meanIncome)**2\n",
    "\n",
    "tmp3 = (c - meanIncome)**2\n",
    "\n",
    "tmp4 = (d - meanIncome)**2\n",
    "\n",
    "tmp5 = (e - meanIncome)**2\n",
    "\n",
    "tmp6 = (f - meanIncome)**2\n",
    "\n",
    "\n",
    "varienceIncome = (tmp1 + tmp2 + tmp3 + tmp4 + tmp5 + tmp6) / (1/6) \n",
    "print(varienceIncome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "original-operation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344337.0441878132\n"
     ]
    }
   ],
   "source": [
    "#finding standard deviation for calulating income\n",
    "import math \n",
    "std = math.sqrt(varienceIncome)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "parallel-cross",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1442385229966044\n",
      "0.16650352990212047\n",
      "0.0009680437785006864\n",
      "0.2710522579801961\n",
      "-0.1732798363516254\n",
      "-0.12100547231258757\n"
     ]
    }
   ],
   "source": [
    "#normalizing income:\n",
    "income1 = (a - meanIncome) / std\n",
    "print(income1)\n",
    "income2 = (b - meanIncome) / std\n",
    "print(income2)\n",
    "income3 = (c - meanIncome) / std\n",
    "print(income3)\n",
    "income4 = (d - meanIncome) / std\n",
    "print(income4)\n",
    "income5 = (e - meanIncome) / std\n",
    "print(income5)\n",
    "income6 = (f - meanIncome) / std\n",
    "print(income6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "incident-afghanistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.666666666666664\n"
     ]
    }
   ],
   "source": [
    "#finding mean for age \n",
    "a2 = 25\n",
    "b2 = 56\n",
    "c2 = 65\n",
    "d2 = 32\n",
    "e2 = 41\n",
    "f2 = 49\n",
    "meanAge = (a2 + b2 + c2 + d2 + e2 + f2) / 6\n",
    "print(meanAge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "different-antarctica",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6728.0\n"
     ]
    }
   ],
   "source": [
    "#finding varience for calulating age\n",
    "tmpAge1 = (a2 - meanAge)**2\n",
    "\n",
    "tmpAge2 = (b2 - meanAge)**2\n",
    "\n",
    "tmpAge3 = (c2 - meanAge)**2\n",
    "\n",
    "tmpAge4 = (d2 - meanAge)**2\n",
    "\n",
    "tmpAge5 = (e2 - meanAge)**2\n",
    "\n",
    "tmpAge6 = (f2 - meanAge)**2\n",
    "\n",
    "\n",
    "varienceAge = (tmpAge1 + tmpAge2 + tmpAge3 + tmpAge4 + tmpAge5 + tmpAge6) / (1/6) \n",
    "print(varienceAge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "antique-commission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.02438661763951\n"
     ]
    }
   ],
   "source": [
    "#finding standard deviation for calulating age\n",
    "stdAge = math.sqrt(varienceAge)\n",
    "print(stdAge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "public-society",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2397660924713006\n",
      "0.13817029057668173\n",
      "0.24789375662287016\n",
      "-0.1544256188798207\n",
      "-0.04470215283363229\n",
      "0.05282981698520186\n"
     ]
    }
   ],
   "source": [
    "#normalizing age\n",
    "Age1 = (a2 - meanAge) / stdAge\n",
    "print(Age1)\n",
    "Age2 = (b2 - meanAge) / stdAge\n",
    "print(Age2)\n",
    "Age3 = (c2 - meanAge) / stdAge\n",
    "print(Age3)\n",
    "Age4 = (d2 - meanAge) / stdAge\n",
    "print(Age4)\n",
    "Age5 = (e2 - meanAge) / stdAge\n",
    "print(Age5)\n",
    "Age6 = (f2 - meanAge) / stdAge\n",
    "print(Age6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-minneapolis",
   "metadata": {},
   "source": [
    "2.9 - Can normalizing the data change which two records are farthest from each other in terms of Euclidean distance?\n",
    "\n",
    "Yes, normalizing can change which two records are farthest from each other in regards to Euclidean distance. This is because there is such a large variation in the income data that when normalizing the data those large variations are reduced. This is an answer I have found by researching how the Euclidean distance is effected by normalizing data, I'm not completely sure if this is correct though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "pressed-doctrine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>107000.004491</td>\n",
       "      <td>50000.016000</td>\n",
       "      <td>143000.000171</td>\n",
       "      <td>10000.012800</td>\n",
       "      <td>8000.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107000.004491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57000.000711</td>\n",
       "      <td>36000.008000</td>\n",
       "      <td>117000.000962</td>\n",
       "      <td>99000.000247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50000.016000</td>\n",
       "      <td>57000.000711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>93000.005855</td>\n",
       "      <td>60000.004800</td>\n",
       "      <td>42000.003048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143000.000171</td>\n",
       "      <td>36000.008000</td>\n",
       "      <td>93000.005855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>153000.000265</td>\n",
       "      <td>135000.001070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000.012800</td>\n",
       "      <td>117000.000962</td>\n",
       "      <td>60000.004800</td>\n",
       "      <td>153000.000265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18000.001778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8000.036000</td>\n",
       "      <td>99000.000247</td>\n",
       "      <td>42000.003048</td>\n",
       "      <td>135000.001070</td>\n",
       "      <td>18000.001778</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0              1             2              3              4  \\\n",
       "0       0.000000  107000.004491  50000.016000  143000.000171   10000.012800   \n",
       "1  107000.004491       0.000000  57000.000711   36000.008000  117000.000962   \n",
       "2   50000.016000   57000.000711      0.000000   93000.005855   60000.004800   \n",
       "3  143000.000171   36000.008000  93000.005855       0.000000  153000.000265   \n",
       "4   10000.012800  117000.000962  60000.004800  153000.000265       0.000000   \n",
       "5    8000.036000   99000.000247  42000.003048  135000.001070   18000.001778   \n",
       "\n",
       "               5  \n",
       "0    8000.036000  \n",
       "1   99000.000247  \n",
       "2   42000.003048  \n",
       "3  135000.001070  \n",
       "4   18000.001778  \n",
       "5       0.000000  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#difference b/w orginal and normalized distance\n",
    "#orginal chart\n",
    "from sklearn.metrics import pairwise\n",
    "d = pairwise.pairwise_distances(df, metric='euclidean')\n",
    "pd.DataFrame(d, index=df.index, columns=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "later-insured",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.935690</td>\n",
       "      <td>3.052916</td>\n",
       "      <td>2.543812</td>\n",
       "      <td>1.183284</td>\n",
       "      <td>1.761101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.935690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.191589</td>\n",
       "      <td>1.864280</td>\n",
       "      <td>2.315215</td>\n",
       "      <td>1.799444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.052916</td>\n",
       "      <td>1.191589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.907409</td>\n",
       "      <td>2.043303</td>\n",
       "      <td>1.380358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.543812</td>\n",
       "      <td>1.864280</td>\n",
       "      <td>2.907409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.746075</td>\n",
       "      <td>2.660809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.183284</td>\n",
       "      <td>2.315215</td>\n",
       "      <td>2.043303</td>\n",
       "      <td>2.746075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.761101</td>\n",
       "      <td>1.799444</td>\n",
       "      <td>1.380358</td>\n",
       "      <td>2.660809</td>\n",
       "      <td>0.663945</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0  0.000000  2.935690  3.052916  2.543812  1.183284  1.761101\n",
       "1  2.935690  0.000000  1.191589  1.864280  2.315215  1.799444\n",
       "2  3.052916  1.191589  0.000000  2.907409  2.043303  1.380358\n",
       "3  2.543812  1.864280  2.907409  0.000000  2.746075  2.660809\n",
       "4  1.183284  2.315215  2.043303  2.746075  0.000000  0.663945\n",
       "5  1.761101  1.799444  1.380358  2.660809  0.663945  0.000000"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalized chart\n",
    "d_norm = pairwise.pairwise_distances(df_norm, metric='euclidean')\n",
    "pd.DataFrame(d_norm, index=df_norm.index, columns=df_norm.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-prefix",
   "metadata": {},
   "source": [
    "2.10 - Two models are applied to a dataset that has been partitioned. Model A is considerably more accurate than model B on the training data, but slightly less accurate than model B on the validation data. Which model are you more likely to consider for final deployment?\n",
    "\n",
    "When looking at these two models we would choose the validation model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-diagnosis",
   "metadata": {},
   "source": [
    "2.11 - The dataset ToyotaCorolla.csv contains data on used cars on sale during the late summer of 2004 in the Netherlands. It has 1436 records containing details on 38 attributes, including Price, Age, Kilometers, HP, and other specifications. We plan to analyze the data using various data mining techniques described in future chapters. Prepare the data for use as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "peripheral-dinner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#This is the answer to question 2.11a\n",
    "#Firstly we need to import the data and then we need to create a dummy variable tht will hold information that within the Fuel_Type and Color. We can do this by using get_dummies.\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "DATA = Path(\"C:/Users/chris/Documents/DS 110-DM/data\")\n",
    "df_toyota = pd.read_csv(DATA / \"ToyotaCorolla.csv\")\n",
    "print(type(df_toyota))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cross-onion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fuel_Type_Diesel  Fuel_Type_Petrol  Color_Black  Color_Blue  Color_Green  \\\n",
      "0                 0                 0            0           0            0   \n",
      "1                 1                 0            0           1            0   \n",
      "2                 0                 0            0           0            0   \n",
      "3                 1                 0            0           0            0   \n",
      "4                 0                 0            0           0            0   \n",
      "\n",
      "   Color_Grey  Color_Red  Color_Silver  Color_Violet  Color_White  \\\n",
      "0           0          0             0             0            0   \n",
      "1           0          0             0             0            0   \n",
      "2           0          0             0             0            0   \n",
      "3           0          0             1             0            0   \n",
      "4           0          0             0             0            0   \n",
      "\n",
      "   Color_Yellow  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n"
     ]
    }
   ],
   "source": [
    "df_toyota = pd.get_dummies(df_toyota.iloc[:, 3:39], prefix_sep='_', drop_first=True)\n",
    "print(df_toyota.loc[:, 'Fuel_Type_Diesel':'Color_Yellow'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-wayne",
   "metadata": {},
   "source": [
    "2.11b - Prepare the dataset (as factored into dummies) for data mining techniques of supervised learning by creating partitions in Python. Select all the variables and use default values for the random seed and partitioning percentages for training (50%), validation (30%), and test (20%) sets. Describe the roles that these partitions will play in modeling.\n",
    "\n",
    "The first partition is training which is used to train a model that has been built. This partition is generally the largest to give the model the most amount of data to train. A good analogy for this would be thinking of this as a lecture\n",
    "The second partition is validation which is used to assess the predictive capabilties of the model that was trained. This data is complete new compared to the training so in a sense this is kind of like giving a quiz to it after its learned. When we uses validation data we know the actual so we are able to test the models to see how accruately they can predicted those actual values.\n",
    "The third partition is testing which is used to test the real performance of the model because as we learned in the book, models can show great performance than others due to a bias in the validation data. This step would be the exam when connecting it back to the analogy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "surprising-holiday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'Model', 'Price', 'Age_08_04', 'Mfg_Month', 'Mfg_Year', 'KM',\n",
       "       'HP', 'Met_Color', 'Automatic', 'CC', 'Doors', 'Cylinders', 'Gears',\n",
       "       'Quarterly_Tax', 'Weight', 'Mfr_Guarantee', 'BOVAG_Guarantee',\n",
       "       'Guarantee_Period', 'ABS', 'Airbag_1', 'Airbag_2', 'Airco',\n",
       "       'Automatic_airco', 'Boardcomputer', 'CD_Player', 'Central_Lock',\n",
       "       'Powered_Windows', 'Power_Steering', 'Radio', 'Mistlamps',\n",
       "       'Sport_Model', 'Backseat_Divider', 'Metallic_Rim', 'Radio_cassette',\n",
       "       'Parking_Assistant', 'Tow_Bar', 'Fuel_Type_CNG', 'Fuel_Type_Diesel',\n",
       "       'Fuel_Type_Petrol', 'Color_Beige', 'Color_Black', 'Color_Blue',\n",
       "       'Color_Green', 'Color_Grey', 'Color_Red', 'Color_Silver',\n",
       "       'Color_Violet', 'Color_White', 'Color_Yellow'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-addition",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
